---
title: "Run Times (and Tips) for Various Functions in `{peacesciencer}`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(tidyverse)
library(peacesciencer)
library(kableExtra)

create_bench <- readRDS("~/Dropbox/projects/peacesciencer/data-raw/times/create_bench.rds")
state_bench <- readRDS("~/Dropbox/projects/peacesciencer/data-raw/times/state_bench.rds")
dyad_bench <- readRDS("~/Dropbox/projects/peacesciencer/data-raw/times/dyad_bench.rds")

```

I had time around watching the UEFA Euro 2020 action to evaluate the expected run times in `{peacesciencer}`. The TV is in the living room and that meant I can have my laptop open for this. My laptop is a more appropriate computer for doing this because [my desktop is comically overpowered](http://svmiller.com/blog/2019/07/notes-to-self-new-linux-installation-r-ubuntu/) and may provide some unrealistic expectations (for me) about how other users might experience `{peacesciencer}`.[^mylaptop]

[^mylaptop]: My laptop is pretty good as far as performance laptops go. At the least, it has 16 GB of RAM. That is on the high end as far as most consumer laptops go, but dedicated professionals may have a laptop similar to what I have.

[A subdirectory on the project's Github](https://github.com/svmiller/peacesciencer/tree/master/data-raw/times) shows what I did here. I grouped the functions in `{peacesciencer}` into two types, with one type have two subcomponents. The first type creates base data. These are `create_statedays()`, `create_stateyears()`, and `create_dyadyears()`. The second type broadly changes data---either adding to it or subtracting from it. For convenience sake, it's good to think of this family of `{peacesciencer}` functions as applicable to state-year or dyad-year data. With that in mind, I used the `{microbenchmark}` package to run each of the relevant functions 100 times---across those two types (and three overall groups)---to see how long these functions can take for a user with a computer similar to mine. The times are calculated as nanoseconds and the benchmarking happened while I was also fiddling with other things, perusing the internet, and watching UEFA Euro 2020. Thus, what I offer here is illustrative, but still useful.

A user can see the associated R Markdown file for this vignette to see the code for processing/formatting, so I want to focus on just the substance. Here is a summary table of the run times for the `create_*` functions from this exercise. Note that these functions were executed with the default options, so these are all Correlates of War state system data from 1816 to the most recently concluded calendar year.

```{r, echo=F, eval=T}

tibble(create_bench) %>%
  mutate(time = time/1e9) %>%
  group_by(expr) %>%
  summarize(mean = mean(time),
            median = median(time),
            interval = NA,
            lwr = quantile(time, .025),
            upr = quantile(time, .975),
            min = min(time),
            max = max(time)) %>%
  mutate_if(is.numeric, ~round(., 3)) %>%
  mutate(interval = paste0("[", lwr,", ", upr, "]")) %>%
  select(-lwr, -upr) %>%
  kbl(., caption = "A Summary of Run Times Across 100 Trials for Data-Creating Functions in `{peacesciencer}`",
      col.names = c("Function", "Average", "Median", "95% Interval", "Minimum", "Maximum"),
      align = c("l", "c","c","c", "c","c"))  %>%
  kable_styling(position = "center", full_width = F, bootstrap_options = "striped") %>%
  column_spec(1, monospace=TRUE)

```

The simulations show that creating dyad-years is by far the most time-intensive data-creating function in `{peacesciencer}`. This is not terribly surprising. The code for `create_dyadyears()` is transforming the raw Correlates of War (or Gleditsch-Ward) state system data. The nature of this transformation is invariably going to take more time than state-year or even state-day summaries of the data. That said, about 4-5 seconds for creating these data is pretty damn good, all things considered.

These are the run times for the functions that add to state-year data, ranged from most time-consuming to least time-consuming.

```{r, echo=F, eval=T}

tibble(state_bench) %>%
  mutate(time = time/1e9) %>%
  group_by(expr) %>%
  summarize(mean = mean(time),
            median = median(time),
            interval = NA,
            lwr = quantile(time, .025),
            upr = quantile(time, .975),
            min = min(time),
            max = max(time)) %>%
  mutate_if(is.numeric, ~round(., 3)) %>%
  mutate(interval = paste0("[", lwr,", ", upr, "]")) %>%
  select(-lwr, -upr) %>%
  arrange(-mean) %>%
  kbl(., caption = "A Summary of Run Times Across 100 Trials for State-Year Data-Changing Functions in `{peacesciencer}`",
      col.names = c("Function", "Average", "Median", "95% Interval", "Minimum", "Maximum"),
      align = c("l", "c","c","c", "c","c"))  %>%
  kable_styling(position = "center", full_width = F, bootstrap_options = "striped") %>%
  column_spec(1, monospace=TRUE)

```

There are five functions for which the average execution time is over a second. Knowing what I know about how I wrote these functions, a few of them make some sense.`add_archigos()` takes the most time by far---an average of over six seconds---largely because 1) it needs to rowwise-transform a subset of the raw data to extend dates into leader-days before calculating the relevant variables as a group-by mutate before doing the most time-consuming function I sometimes bury into these functions: a group-by slice for eliminating duplicates. `add_creg_fractionalization()` has this same group-by slice largely because its state codes are not quite Correlates of War and note quite Gleditsch-Ward, for which a group-by slice is one of my go-tos for eliminating grouped duplicates. `add_capital_distance()` is a bit time-consuming because it's doing on-the-fly "as the crow flies" distance estimates between state capitals using the provided latitude/longitude coordinates. `add_strategic_rivalries()` doesn't have any of these, but it has a lot of buried if-elses for how a user may want to calculate the presence of a rivalry type at the state-year.

The dyad-year run times are a little bit more interesting and will merit some further explanation. Most of these are a little time-consuming because of the reasons mentioned above (e.g. group-by slices, as in the `add_creg_fractionalization()` and `add_archigos()` cases). The peace-year calculations are a little time-consuming, but ultimately have a straightforward explanation.

```{r, echo=F, eval=T}

tibble(dyad_bench) %>%
  mutate(time = time/1e9) %>%
  group_by(expr) %>%
  summarize(mean = mean(time),
            median = median(time),
            interval = NA,
            lwr = quantile(time, .025),
            upr = quantile(time, .975),
            min = min(time),
            max = max(time)) %>%
  mutate_if(is.numeric, ~round(., 3)) %>%
  mutate(interval = paste0("[", lwr,", ", upr, "]")) %>%
  select(-lwr, -upr) %>%
  arrange(-mean) %>%
  kbl(., caption = "A Summary of Run Times Across 100 Trials for Dyad-Year Data-Changing Functions in `{peacesciencer}`",
      col.names = c("Function", "Average", "Median", "95% Interval", "Minimum", "Maximum"),
      align = c("l", "c","c","c", "c","c"))  %>%
  kable_styling(position = "center", full_width = F, bootstrap_options = "striped") %>%
  column_spec(1, monospace=TRUE)

```

<!-- It may raise a curious eyebrow that the `add_cow_trade()` function takes over 20 seconds (on average) for dyad-year data. The explanation is simple, if a bit strange. The raw data for these are like over 10 MBs even in their compressed form. For context: CRAN only allows *5* MBs for *total* package size, barring an exception from CRAN (which it almost assuredly would not give me because this package is not that important in the broad R ecosystem). That means, if you stare closely at the code, you'll see the function *loads the data from my website* and then processes it further. It means the function will not work for dyad-year analyses if you don't have an active internet connection. It also means you'll want a stable internet connection. Part of these summaries may be a function of how spotty my wireless connection is in my living room (and how I was also streaming UEFA Euro 2020). If you think you have a more flexible solution here, feel free to reach out because this does kind of bug me (even as it's CRAN compliant, which is a high priority for me). -->

Basically, `add_peace_years()` works *generally* with a variety of data types you feed it. It's also implicitly a grouped function. For state-year data, that means you have about 217 "groups" (i.e. states) in the Correlates of War cases. If you want---as I do here---the full damn universe of Correlates of War dyads from 1816 to 2020, that means you'll have 41,252 dyads for which `add_peace_years()` will calculate your peace spells. So yeah, that's going to take some time. You can cut that in about half if you filtered the data to just politically relevant dyads before calculating peace years.

Ultimately, [the examples on the README](http://svmiller.com/peacesciencer/index.html) show that you can do most things in `{peacesciencer}` in a matter of seconds. Unless you're stress-testing the package's ability to do everything on the full universe of dyad-year data, you can create the kind of data you want in well under a minute. Some functions take longer than others, mostly because of some hacks I built into these functions on the premise that I know they'll work as I intend them to work (even if a more optimal alternative is possible).
